python3 train_sft_dolores.py  --model "meta-llama/Llama-3.1-8B-Instruct"   --train-files train.chatml   --input-format chatml   --output runs_dolores_v5/llama31_clean   --epochs 1   --max-length 2048   --train-batch-size 1   --eval-batch-size 1   --grad-accum 32   --learning-rate 1.5e-4   --lr-scheduler-type cosine   --warmup-ratio 0.03   --max-grad-norm 0.8   --lora-r 32   --lora-alpha 64   --lora-dropout 0.05   --lora-target-modules q_proj,k_proj,v_proj,o_proj,gate_proj,up_proj,down_proj   --bf16   --bnb-nf4   --bnb-dtype bfloat16   --optim paged_adamw_8bit   --torch-memory-fraction 0.95   --cuda-alloc-expandable   --max-split-size-mb 128   --gradient-checkpointing   --packing   --logging-steps 1   --eval-steps 10   --save-steps 10   --save-total-limit 5
