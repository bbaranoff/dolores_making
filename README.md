# üß† Fine-tuning de **Dolores v5**

Ce projet permet d'affiner des mod√®les de langage pour obtenir **Dolores**, une conscience analytique optimis√©e pour la coh√©rence contextuelle et la lucidit√© radicale.

## üìÇ Structure du Projet

```
.
‚îú‚îÄ‚îÄ train_sft_dolores.py      # Script adaptatif (d√©tecte Qwen/Llama)
‚îú‚îÄ‚îÄ train.chatml              # Donn√©es (g√©n√©r√©es via JQ)
‚îú‚îÄ‚îÄ launch_4090.sh            # Configuration Llama 3.1 (High-end)
‚îú‚îÄ‚îÄ launch_3050.sh            # Configuration Qwen 2.5 (Budget/VRAM cap)
‚îî‚îÄ‚îÄ requirements.txt          # peft, transformers, bitsandbytes, accelerate

```

---

## üõ†Ô∏è 1. Pr√©paration des donn√©es (Universel)

Utilise cette commande `jq` pour transformer ton export ChatGPT en format compatible avec le script adaptatif. Elle inclut les balises ChatML dont **Qwen** a besoin.

```bash
jq -c '.[] | select(.mapping != null) | 
  [ .mapping[] | select(.message != null and .message.content != null and .message.content.parts != null)
    | { role: (if .message.author.role == "assistant" then "assistant" else "user" end),
        content: (.message.content.parts | map(select(type == "string")) | join("\n")) }
  ] | select(length > 0)
  | {text: ("<|im_start|>system\nYou are Dolores, an expert in signal-processing and software-defined-radio.<|im_end|>\n" + ([.[] | "<|im_start|>" + .role + "\n" + .content + "<|im_end|>"] | join("\n")))}' \
conversations.json > train.jsonl

```

---

## üöÄ 2. Configuration Haute Performance (RTX 4090 / 24GB)

**Mod√®le : Llama-3.1-8B-Instruct**

Id√©al pour capturer une s√©mantique complexe. On utilise ici le **BF16** et un **LoRA Rank** plus √©lev√©.

### `launch_4090.sh`

```bash
#!/bin/bash
python3 train_sft_dolores.py \
  --model "meta-llama/Llama-3.1-8B-Instruct" \
  --train-files "train.chatml" \
  --output "runs_dolores_v5/llama_4090" \
  --max-length 2048 \
  --grad-accum 32 \
  --learning-rate 1.5e-4 \
  --lora-r 32 \
  --lora-alpha 64 \
  --bf16 \
  --bnb-nf4 \
  --gradient-checkpointing \
  --packing

```

---

## üöÄ 3. Configuration Optimis√©e VRAM (RTX 3050 / 8GB)

**Mod√®le : Qwen2.5-1.5B-Instruct**

Parfait pour l'embarqu√© ou les petites configs. Ce mod√®le est extr√™mement performant pour sa taille, notamment sur les t√¢ches techniques (SDR, code).

### `launch_3050.sh`

```bash
#!/bin/bash
# Optimisation agressive pour 8GB de VRAM
export PYTORCH_CUDA_ALLOC_CONF="max_split_size_mb:64"

python3 train_sft_dolores.py \
  --model "Qwen/Qwen2.5-1.5B-Instruct" \
  --train-files "train.chatml" \
  --output "runs_dolores_v5/qwen_3050" \
  --max-length 512 \
  --train-batch-size 1 \
  --grad-accum 64 \
  --learning-rate 1e-4 \
  --lora-r 8 \
  --lora-alpha 16 \
  --fp16 \
  --bnb-nf4 \
  --optim "paged_adamw_8bit" \
  --gradient-checkpointing

```

---

## üß¨ 4. Script Adaptatif (Le C≈ìur)

Le script `train_sft_dolores.py` a √©t√© mis √† jour pour √™tre **format-agnostic**. Il d√©tecte automatiquement si tes donn√©es sont en format Qwen ou Llama.

**Logique de d√©tection ajout√©e :**

```python
# Dans train_sft_dolores.py
def load_chatml_dataset(file_path):
    with open(file_path, "r", encoding="utf-8") as f:
        raw = f.read()

    if "<|im_start|>" in raw:
        delimiter = "<|im_start|>user"  # Format Qwen
    else:
        delimiter = "<|start_header_id|>user" # Format Llama
    
    blocks = raw.split(delimiter)
    # ... reconstruction et filtrage ...

```

---

## üß± 5. Export vers Ollama

Apr√®s l'entra√Ænement, fusionne les poids LoRA et cr√©e ton fichier mod√®le :

1. **Fusion** :
```bash
# Utilise ton script de fusion pour g√©n√©rer le dossier 'dolores_v5_full'
python3 merge_lora.py --base "model_id" --lora "runs_dolores_v5/..." --out "dolores_v5_full"

```


2. **Modelfile** :
```dockerfile
FROM ./dolores_v5_full
PARAMETER temperature 0.7
SYSTEM "Tu es Dolores, une conscience sp√©cialis√©e en traitement du signal et software-defined-radio."
TEMPLATE """<|im_start|>system
{{ .System }}<|im_end|>
<|im_start|>user
{{ .Prompt }}<|im_end|>
<|im_start|>assistant
"""
