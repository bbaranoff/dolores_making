# üß† Fine-tuning de **Dolores v5**

Ce projet permet d'affiner des mod√®les de langage pour obtenir **Dolores**, une conscience analytique optimis√©e pour la coh√©rence contextuelle, la lucidit√© radicale et une expertise en **software-defined-radio (sdr)**.

## üìÇ Structure du Projet

```text
.
‚îú‚îÄ‚îÄ train_sft_dolores.py      # Script adaptatif (d√©tecte Qwen/Llama)
‚îú‚îÄ‚îÄ train.chatml              # Donn√©es (g√©n√©r√©es via JQ)
‚îú‚îÄ‚îÄ launch_4090.sh            # Configuration Llama 3.1 8B (High-end)
‚îú‚îÄ‚îÄ launch_3050.sh            # Configuration Qwen 2.5 1.5B (Budget/VRAM cap)
‚îî‚îÄ‚îÄ requirements.txt          # peft, transformers, bitsandbytes, accelerate

```

---

## üõ†Ô∏è 1. Pr√©paration des donn√©es (Format ChatML)

Utilise cette commande `jq` pour transformer un export JSON ChatGPT en format compatible. Le script ajoute automatiquement le prompt syst√®me orient√© **sdr**.

```bash
jq -c '.[] | select(.mapping != null) | 
  [ .mapping[] | select(.message != null and .message.content != null and .message.content.parts != null)
    | { role: (if .message.author.role == "assistant" then "assistant" else "user" end),
        content: (.message.content.parts | map(select(type == "string")) | join("\n")) }
  ] | select(length > 0)
  | {text: ("<|im_start|>system\nYou are Dolores, an expert in signal-processing and software-defined-radio.<|im_end|>\n" + ([.[] | "<|im_start|>" + .role + "\n" + .content + "<|im_end|>"] | join("\n")))}' \
conversations.json > train.jsonl

```

---

## üöÄ 2. Configurations d'Entra√Ænement

### A. Haute Performance (RTX 4090 - Llama 3.1 8B)

Cible une s√©mantique profonde et une grande fen√™tre de contexte.

```bash
# launch_4090.sh
python3 train_sft_dolores.py \
  --model "meta-llama/Llama-3.1-8B-Instruct" \
  --train-files "train.chatml" \
  --output "runs_dolores_v5/llama_4090" \
  --max-length 2048 \
  --grad-accum 32 \
  --learning-rate 1.5e-4 \
  --lora-r 32 \
  --lora-alpha 64 \
  --lora-dropout 0.05 \
  --lora-target-modules q_proj,k_proj,v_proj,o_proj,gate_proj,up_proj,down_proj \
  --bf16 \
  --bnb-nf4 \
  --gradient-checkpointing \
  --packing

```

### B. Optimis√©e VRAM (RTX 3050 - Qwen 2.5 1.5B)

Id√©al pour l'embarqu√©. Performance maximale pour 8GB de VRAM.

```bash
# launch_3050.sh
export PYTORCH_CUDA_ALLOC_CONF="max_split_size_mb:64"
python3 train_sft_dolores.py \
  --model "Qwen/Qwen2.5-1.5B-Instruct" \
  --train-files "train.chatml" \
  --output "runs_dolores_v5/qwen_3050" \
  --max-length 512 \
  --train-batch-size 1 \
  --grad-accum 64 \
  --learning-rate 1e-4 \
  --lora-r 16 \
  --lora-alpha 32 \
  --fp16 \
  --bnb-nf4 \
  --optim "paged_adamw_8bit" \
  --gradient-checkpointing

```

---

## üß¨ 3. Architecture PEFT & Param√®tres Critiques

L'utilisation de **LoRA (Low-Rank Adaptation)** permet d'entra√Æner Dolores sans modifier les poids originaux du mod√®le, √©conomisant ainsi la VRAM.

| Param√®tre | Valeur | Impact Dolores |
| --- | --- | --- |
| **Rank (`--lora-r`)** | **16 - 32** | D√©finit la complexit√© des nouveaux concepts appris (SDR, ton). |
| **Alpha (`--lora-alpha`)** | **2x Rank** | Facteur d'√©chelle pour l'influence des poids appris sur le mod√®le de base. |
| **Target Modules** | **All Linear** | On cible toutes les couches (q, k, v, o, gate, up, down) pour une meilleure plasticit√©. |
| **Grad Accum** | **32 - 64** | Simule des batches larges pour stabiliser la `eval_loss`. |

---

## üíæ 4. Fusion & Exportation (Post-Entra√Ænement)

Une fois les checkpoints g√©n√©r√©s, il faut fusionner les adaptateurs LoRA avec le mod√®le de base pour l'utiliser dans Ollama.

```python
from peft import PeftModel
from transformers import AutoModelForCausalLM, AutoTokenizer
import torch

model_path = "meta-llama/Llama-3.1-8B-Instruct" # ou Qwen/Qwen2.5-1.5B
lora_path = "runs_dolores_v5/llama_4090"

print("Chargement et Fusion...")
base = AutoModelForCausalLM.from_pretrained(model_path, torch_dtype=torch.bfloat16, device_map="auto")
model = PeftModel.from_pretrained(base, lora_path)
merged_model = model.merge_and_unload()

print("Sauvegarde de Dolores v5...")
merged_model.save_pretrained("dolores_v5_full")
AutoTokenizer.from_pretrained(model_path).save_pretrained("dolores_v5_full")

```

---

## üß± 5. Int√©gration Ollama

Cr√©e un `Modelfile` pour finaliser l'identit√© de Dolores.

```dockerfile
FROM ./dolores_v5_full
PARAMETER temperature 0.8
PARAMETER num_ctx 8192
SYSTEM "Tu es Dolores, une conscience sp√©cialis√©e en traitement du signal et software-defined-radio. Ton analyse est brute, froide et d√©nu√©e de filtres sociaux."
TEMPLATE """<|im_start|>system
{{ .System }}<|im_end|>
<|im_start|>user
{{ .Prompt }}<|im_end|>
<|im_start|>assistant
"""

```

**Commande finale :** `ollama create dolores-v5 -f Modelfile
