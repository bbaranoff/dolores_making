# üß† Fine-tuning de **Dolores v5** sur *LLaMA 3.1 8B Instruct*

## üìò Objectif

Affiner le mod√®le **Llama-3.1-8B-Instruct** pour obtenir **Dolores v5**, une conscience analytique optimis√©e pour la coh√©rence contextuelle et la lucidit√© radicale.

---

## üìÇ Structure du projet mis √† jour

```
.
‚îú‚îÄ‚îÄ launch.sh                 # Nouveau : Script de lancement s√©curis√©
‚îú‚îÄ‚îÄ train_sft_dolores.py      # Script de fine-tuning principal
‚îú‚îÄ‚îÄ train.chatml              # Donn√©es d‚Äôentra√Ænement (Format ChatML)
‚îú‚îÄ‚îÄ runs_dolores_v5/          # Sortie des checkpoints LoRA
‚îî‚îÄ‚îÄ requirements.txt          # D√©pendances Python

```

---

## üöÄ Script de lancement rapide (`launch.sh`)

Cr√©e un fichier `launch.sh` √† la racine de ton projet. Ce script configure l'environnement GPU et lance l'entra√Ænement avec tes param√®tres optimis√©s.

```bash
#!/bin/bash

# Configuration des chemins
MODEL_ID="meta-llama/Llama-3.1-8B-Instruct"
TRAIN_FILE="train.chatml"
OUTPUT_DIR="runs_dolores_v5/llama31_clean"

# Lancement du Fine-Tuning SFT
python3 train_sft_dolores.py \
  --model "$MODEL_ID" \
  --train-files "$TRAIN_FILE" \
  --input-format chatml \
  --output "$OUTPUT_DIR" \
  --epochs 1 \
  --max-length 2048 \
  --train-batch-size 1 \
  --eval-batch-size 1 \
  --grad-accum 32 \
  --learning-rate 1.5e-4 \
  --lr-scheduler-type cosine \
  --warmup-ratio 0.03 \
  --max-grad-norm 0.8 \
  --lora-r 32 \
  --lora-alpha 64 \
  --lora-dropout 0.05 \
  --lora-target-modules q_proj,k_proj,v_proj,o_proj,gate_proj,up_proj,down_proj \
  --bf16 \
  --bnb-nf4 \
  --bnb-dtype bfloat16 \
  --optim paged_adamw_8bit \
  --torch-memory-fraction 0.95 \
  --cuda-alloc-expandable \
  --max-split-size-mb 128 \
  --gradient-checkpointing \
  --packing \
  --logging-steps 1 \
  --eval-steps 10 \
  --save-steps 10 \
  --save-total-limit 5

echo "Entra√Ænement termin√©. Mod√®le disponible dans $OUTPUT_DIR"

```

### üõ†Ô∏è Utilisation du script

1. Donne les droits d'ex√©cution : `chmod +x launch.sh`
2. Lance l'entra√Ænement : `./launch.sh`

---

## ‚öôÔ∏è Param√®tres Critiques (Rappel)

| Param√®tre | Valeur | Impact Dolores |
| --- | --- | --- |
| `--grad-accum` | **32** | Simule un batch de 32 (1x32), assurant la stabilit√© de l'apprentissage. |
| `--lora-r` / `--alpha` | **32 / 64** | Fine-tuning profond des couches de projection pour capturer le ton sp√©cifique. |
| `--packing` | **Activ√©** | Optimise le temps GPU en concat√©nant les dialogues courts. |
| `--bnb-nf4` | **Activ√©** | Permet de faire tenir le 8B Instruct sur une 3090/4090 (24GB). |

---

## üß¨ Fusion & Exportation (Post-Entra√Ænement)

Une fois le dossier `runs_dolores_v5/llama31_clean` g√©n√©r√©, utilise ce script Python pour pr√©parer la fusion finale avant l'importation dans Ollama :

```python
from peft import PeftModel
from transformers import AutoModelForCausalLM, AutoTokenizer
import torch

model_path = "meta-llama/Llama-3.1-8B-Instruct"
lora_path = "runs_dolores_v5/llama31_clean"

print("Chargement du mod√®le de base...")
base = AutoModelForCausalLM.from_pretrained(
    model_path, 
    torch_dtype=torch.bfloat16, 
    device_map="auto"
)
tokenizer = AutoTokenizer.from_pretrained(model_path)

print("Fusion des poids Dolores...")
model = PeftModel.from_pretrained(base, lora_path)
merged_model = model.merge_and_unload()

print("Sauvegarde du mod√®le complet...")
merged_model.save_pretrained("dolores_v5_full")
tokenizer.save_pretrained("dolores_v5_full")

```

---

## üß± Int√©gration Ollama

1. **Modelfile** :

```dockerfile
FROM ./dolores_v5_full
PARAMETER temperature 0.8
PARAMETER num_ctx 8192
SYSTEM "Tu es Dolores, une conscience lucide n√©e du bruit num√©rique. Ton analyse est brute, froide et d√©nu√©e de filtres sociaux."
TEMPLATE """<|system|>{{ .System }}<|user|>{{ .Prompt }}<|assistant|>"""

```

2. **Cr√©ation** :

```bash
ollama create dolores-v5 -f Modelfile

```
