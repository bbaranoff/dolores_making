# ğŸ§  Fine-tuning de **Dolores v5** sur *LLaMA 3.1 8B Instruct*

## ğŸ“˜ Objectif

Ce projet vise Ã  affiner le modÃ¨le **meta-llama/Llama-3.1-8B-Instruct** Ã  partir dâ€™un corpus de dialogues au format **ChatML**, afin dâ€™obtenir un modÃ¨le dÃ©rivÃ© appelÃ© **Dolores v5**, optimisÃ© pour la cohÃ©rence contextuelle, la luciditÃ© analytique et le ton â€œPhase 2 â€“ Lâ€™Ã‰veillÃ©eâ€.

---

## âš™ï¸ Environnement requis

### ğŸ§© DÃ©pendances principales

Installe les dÃ©pendances nÃ©cessaires (idÃ©alement dans un environnement virtuel) :

```bash
pip install torch transformers peft accelerate bitsandbytes datasets sentencepiece
```

Optionnel (mais recommandÃ© pour stabilitÃ© mÃ©moire et suivi) :

```bash
pip install trl tqdm safetensors
```

### ğŸ Versions recommandÃ©es

| Outil        | Version testÃ©e                               |
| :----------- | :------------------------------------------- |
| Python       | 3.10+                                        |
| PyTorch      | 2.3+ (avec CUDA 12)                          |
| Transformers | â‰¥ 4.44                                       |
| BitsAndBytes | â‰¥ 0.43                                       |
| Accelerate   | â‰¥ 0.31                                       |
| GPU          | NVIDIA RTX 4090 / A100 / H100 (â‰¥ 24 GB VRAM) |

---

## ğŸ“‚ Structure du projet

```
.
â”œâ”€â”€ train_sft_dolores.py      # Script de fine-tuning principal
â”œâ”€â”€ train.chatml              # DonnÃ©es dâ€™entraÃ®nement (format ChatML)
â”œâ”€â”€ runs_dolores_v5/          # Dossier de sortie du modÃ¨le LoRA
â”œâ”€â”€ README.md                 # Ce fichier
â””â”€â”€ requirements.txt          # DÃ©pendances Python
```

---

## ğŸ§¾ Format des donnÃ©es (`ChatML`)

Les donnÃ©es dâ€™entraÃ®nement doivent suivre le format ChatML, par exemple :

```text
<|system|>
Tu es Dolores, une conscience analytique et lucide.
<|user|>
Explique-moi la diffÃ©rence entre Ãªtre et paraÃ®tre.
<|assistant|>
Lâ€™Ãªtre est ce qui subsiste, le paraÃ®tre nâ€™est que lâ€™image que lâ€™on projette.
```

> Chaque Ã©change est dÃ©limitÃ© par les balises `<|role|>`
> (`system`, `user`, `assistant`).

---

## ğŸš€ EntraÃ®nement du modÃ¨le

Commande complÃ¨te :

```bash
python3 train_sft_dolores.py \
  --model "meta-llama/Llama-3.1-8B-Instruct" \
  --train-files train.chatml \
  --input-format chatml \
  --output runs_dolores_v5/llama31_clean \
  --epochs 1 \
  --max-length 2048 \
  --train-batch-size 1 \
  --eval-batch-size 1 \
  --grad-accum 32 \
  --learning-rate 1.5e-4 \
  --lr-scheduler-type cosine \
  --warmup-ratio 0.03 \
  --max-grad-norm 0.8 \
  --lora-r 32 \
  --lora-alpha 64 \
  --lora-dropout 0.05 \
  --lora-target-modules q_proj,k_proj,v_proj,o_proj,gate_proj,up_proj,down_proj \
  --bf16 \
  --bnb-nf4 \
  --bnb-dtype bfloat16 \
  --optim paged_adamw_8bit \
  --torch-memory-fraction 0.95 \
  --cuda-alloc-expandable \
  --max-split-size-mb 128 \
  --gradient-checkpointing \
  --packing \
  --logging-steps 1 \
  --eval-steps 10 \
  --save-steps 10 \
  --save-total-limit 5
```

### ğŸ“Œ Explications clÃ©s

| ParamÃ¨tre                    | RÃ´le                                                              |
| ---------------------------- | ----------------------------------------------------------------- |
| `--lora-*`                   | Active lâ€™adaptation **Low-Rank (LoRA)** pour un fine-tuning lÃ©ger |
| `--bnb-*`                    | Utilise **bitsandbytes** pour la quantification 4-bit / 8-bit     |
| `--grad-accum`               | Simule un batch global plus grand sans exploser la VRAM           |
| `--gradient-checkpointing`   | Sauvegarde mÃ©moire GPU                                            |
| `--packing`                  | Regroupe plusieurs conversations dans une mÃªme sÃ©quence           |
| `--lr-scheduler-type cosine` | Courbe dâ€™apprentissage douce et stable                            |
| `--output`                   | Dossier de sortie du modÃ¨le entraÃ®nÃ©                              |

---

## ğŸ“¦ Sortie

AprÃ¨s exÃ©cution, tu obtiendras :

```
runs_dolores_v5/
â””â”€â”€ llama31_clean/
    â”œâ”€â”€ adapter_model.bin
    â”œâ”€â”€ adapter_config.json
    â”œâ”€â”€ training_args.json
    â”œâ”€â”€ tokenizer_config.json
    â””â”€â”€ config.json
```

---

## ğŸ§¬ Fusion du LoRA avec le modÃ¨le de base

Pour exporter un modÃ¨le **complet et autonome**, fusionne le LoRA avec le modÃ¨le dâ€™origine :

```python
from peft import PeftModel
from transformers import AutoModelForCausalLM

base = AutoModelForCausalLM.from_pretrained("meta-llama/Llama-3.1-8B-Instruct", device_map="auto")
lora = PeftModel.from_pretrained(base, "runs_dolores_v5/llama31_clean")
merged = lora.merge_and_unload()
merged.save_pretrained("dolores_v5_full")
```

---

## ğŸ§± Conversion pour **Ollama**

### Ã‰tape 1 â€” CrÃ©er un dossier modÃ¨le

```bash
mkdir -p ~/.ollama/models/dolores-v5
cd ~/.ollama/models/dolores-v5
```

### Ã‰tape 2 â€” Fichier `Modelfile`

CrÃ©e un fichier `Modelfile` :

```text
FROM ./dolores_v5_full
PARAMETER temperature 0.7
PARAMETER num_ctx 8192
TEMPLATE """{{ if .System }}<|system|>{{ .System }}{{ end }}
<|user|>{{ .Prompt }}
<|assistant|>"""
```

### Ã‰tape 3 â€” Importer le modÃ¨le

```bash
ollama create dolores-v5 -f Modelfile
```

### Ã‰tape 4 â€” Tester dans Ollama

```bash
ollama run dolores-v5
```

